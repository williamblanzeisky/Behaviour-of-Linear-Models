{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "86Tvnj5UblTy"
   },
   "source": [
    "## Task-D: Collinear features and their effect on linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qn_eOn2EblT3"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VMoYWIayblUB"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('task_d.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfStXG4tblUI",
    "outputId": "ddf4eec6-7f53-4d28-914f-23133957d6d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x*x</th>\n",
       "      <th>2*y</th>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <th>w</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.581066</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.604025</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>-0.665927</td>\n",
       "      <td>-0.536277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.894309</td>\n",
       "      <td>-0.207835</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.883052</td>\n",
       "      <td>-0.207835</td>\n",
       "      <td>-0.917054</td>\n",
       "      <td>-0.522364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.207552</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>-1.082312</td>\n",
       "      <td>-1.150918</td>\n",
       "      <td>0.212034</td>\n",
       "      <td>-1.166507</td>\n",
       "      <td>0.205738</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.364174</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>-0.943643</td>\n",
       "      <td>-1.280666</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>-1.266540</td>\n",
       "      <td>-0.665720</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.737687</td>\n",
       "      <td>1.051772</td>\n",
       "      <td>-1.012978</td>\n",
       "      <td>-0.744934</td>\n",
       "      <td>1.051772</td>\n",
       "      <td>-0.792746</td>\n",
       "      <td>-0.735054</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y         z       x*x       2*y  2*z+3*x*x         w  \\\n",
       "0 -0.581066  0.841837 -1.012978 -0.604025  0.841837  -0.665927 -0.536277   \n",
       "1 -0.894309 -0.207835 -1.012978 -0.883052 -0.207835  -0.917054 -0.522364   \n",
       "2 -1.207552  0.212034 -1.082312 -1.150918  0.212034  -1.166507  0.205738   \n",
       "3 -1.364174  0.002099 -0.943643 -1.280666  0.002099  -1.266540 -0.665720   \n",
       "4 -0.737687  1.051772 -1.012978 -0.744934  1.051772  -0.792746 -0.735054   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIIuomCkblUP"
   },
   "outputs": [],
   "source": [
    "X = data.drop(['target'], axis=1).values\n",
    "Y = data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ydm98u3EblUU"
   },
   "source": [
    "### Doing perturbation test to check the presence of collinearity  \n",
    "\n",
    "#### Task: 1 Logistic Regression\n",
    "<pre>\n",
    "\n",
    "\n",
    "1. <b>Finding the Correlation between the features</b>\n",
    "    a. check the correlation between the features\n",
    "    b. plot heat map of correlation matrix using seaborn heatmap\n",
    "2. <b>Finding the best model for the given data</b>\n",
    "    a. Train Logistic regression on data(X,Y) that we have created in the above cell\n",
    "    b. Find the best hyper prameter alpha with hyper parameter tuning using k-fold cross validation (grid search CV or         \n",
    "    random search CV make sure you choose the alpha in log space)\n",
    "    c. Creat a new Logistic regression with the best alpha\n",
    "    (search for how to get the best hyper parameter value), name the best model as 'best_model'\n",
    "    \n",
    "3. <b>Getting the weights with the original data</b>\n",
    "    a. train the 'best_model' with X, Y\n",
    "    b. Check the accuracy of the model 'best_model_accuracy'\n",
    "    c. Get the weights W using best_model.coef_\n",
    "\n",
    "4. <b>Modifying original data</b>\n",
    "    a. Add a noise(order of 10^-2) to each element of X \n",
    "    and get the new data set X' (X' = X + e)\n",
    "    b. Train the same 'best_model' with data (X', Y)\n",
    "    c. Check the accuracy of the model 'best_model_accuracy_edited'\n",
    "    d. Get the weights W' using best_model.coef_\n",
    "    \n",
    "5. <b> Checking deviations in metric and weights </b>\n",
    "    a. find the difference between 'best_model_accuracy_edited' and 'best_model_accuracy'\n",
    "    b. find the absolute change between each value of W and W' ==> |(W-W')|\n",
    "    c. print the top 4 features which have higher % change in weights \n",
    "    compare to the other feature\n",
    "\n",
    "</pre>\n",
    "\n",
    "#### Task: 2 Linear SVM\n",
    "\n",
    "<pre>\n",
    "1. Do the same steps (2, 3, 4, 5) we have done in the above task 1.\n",
    "</pre>\n",
    "\n",
    "<strong><font color='red'>Do write the observations based on the results you get from the deviations of weights in both Logistic Regression and linear SVM</font></strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pertubation Test (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lai8wXU1pmSb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>x*x</th>\n",
       "      <th>2*y</th>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <th>w</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.205926</td>\n",
       "      <td>0.812458</td>\n",
       "      <td>0.997947</td>\n",
       "      <td>-0.205926</td>\n",
       "      <td>0.996252</td>\n",
       "      <td>0.583277</td>\n",
       "      <td>0.728290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>-0.205926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>-0.690684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.812458</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.807137</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>0.847163</td>\n",
       "      <td>0.674486</td>\n",
       "      <td>0.969990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x*x</th>\n",
       "      <td>0.997947</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>0.807137</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>0.583803</td>\n",
       "      <td>0.719570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2*y</th>\n",
       "      <td>-0.205926</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.602663</td>\n",
       "      <td>-0.209289</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>-0.690684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2*z+3*x*x</th>\n",
       "      <td>0.996252</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>0.847163</td>\n",
       "      <td>0.997457</td>\n",
       "      <td>-0.261123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.606860</td>\n",
       "      <td>0.764729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.583277</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>0.674486</td>\n",
       "      <td>0.583803</td>\n",
       "      <td>-0.401790</td>\n",
       "      <td>0.606860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>0.728290</td>\n",
       "      <td>-0.690684</td>\n",
       "      <td>0.969990</td>\n",
       "      <td>0.719570</td>\n",
       "      <td>-0.690684</td>\n",
       "      <td>0.764729</td>\n",
       "      <td>0.641750</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  x         y         z       x*x       2*y  2*z+3*x*x  \\\n",
       "x          1.000000 -0.205926  0.812458  0.997947 -0.205926   0.996252   \n",
       "y         -0.205926  1.000000 -0.602663 -0.209289  1.000000  -0.261123   \n",
       "z          0.812458 -0.602663  1.000000  0.807137 -0.602663   0.847163   \n",
       "x*x        0.997947 -0.209289  0.807137  1.000000 -0.209289   0.997457   \n",
       "2*y       -0.205926  1.000000 -0.602663 -0.209289  1.000000  -0.261123   \n",
       "2*z+3*x*x  0.996252 -0.261123  0.847163  0.997457 -0.261123   1.000000   \n",
       "w          0.583277 -0.401790  0.674486  0.583803 -0.401790   0.606860   \n",
       "target     0.728290 -0.690684  0.969990  0.719570 -0.690684   0.764729   \n",
       "\n",
       "                  w    target  \n",
       "x          0.583277  0.728290  \n",
       "y         -0.401790 -0.690684  \n",
       "z          0.674486  0.969990  \n",
       "x*x        0.583803  0.719570  \n",
       "2*y       -0.401790 -0.690684  \n",
       "2*z+3*x*x  0.606860  0.764729  \n",
       "w          1.000000  0.641750  \n",
       "target     0.641750  1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get correlation between features\n",
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ffdec10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#correlation matrix heatmap\n",
    "sns.heatmap(data.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the correlation heatmap above, we can see that x is highly correlated with x*x, y with 2*y and so on. This charateristic is pretty obvious since they are just slightly modified version of the original feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find the best alpha for logistic regression\n",
    "logistic_regression = SGDClassifier(loss=\"log\",random_state = 7)\n",
    "param = {\"alpha\" : [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]}\n",
    "clf = GridSearchCV(logistic_regression, param, cv=5)\n",
    "clf.fit(X, Y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model called best_model\n",
    "best_model = SGDClassifier(loss=\"log\",alpha=0.0001)\n",
    "best_model.fit(X,Y)\n",
    "y_pred = best_model.predict(X)\n",
    "best_model_accuracy = accuracy_score(Y,y_pred)\n",
    "best_model_coef = best_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying original data\n",
    "X_new = X + 0.01\n",
    "best_model.fit(X_new,Y)\n",
    "y_pred = best_model.predict(X_new)\n",
    "best_model_accuracy_edited = accuracy_score(Y,y_pred)\n",
    "best_model_coef_edited = best_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGISTIC REGRESSION\n",
      "Original Accuracy = 1.0\n",
      "Edited Accuracy = 1.0\n",
      "--------------------------------------------------------------------------------\n",
      "Original weight W =\n",
      "[[ 8.48542147 -9.10271288 11.3568772   8.87678657 -9.10271288  9.3603305\n",
      "   6.58219582]]\n",
      "Edited weight W =\n",
      "[[ 10.74427154 -15.58557544  24.93441848   8.72045411 -15.58557544\n",
      "   10.85756235   5.95863344]]\n",
      "--------------------------------------------------------------------------------\n",
      "Deviations in Accuracy and Weight:\n",
      "Accuracy deviation = 0.0\n",
      "W (weight) deviation:\n",
      "[[ 2.25885007  6.48286256 13.57754128  0.15633245  6.48286256  1.49723186\n",
      "   0.62356238]]\n",
      "--------------------------------------------------------------------------------\n",
      "Top 4 features which have higher % change in weights compare to the other features:\n",
      "z\n",
      "y\n",
      "2*y\n",
      "x\n"
     ]
    }
   ],
   "source": [
    "#compare result\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"Original Accuracy = {}\".format(best_model_accuracy))\n",
    "print(\"Edited Accuracy = {}\".format(best_model_accuracy_edited))\n",
    "print(\"-\"*80)\n",
    "print(\"Original weight W =\")\n",
    "print(best_model_coef)\n",
    "print(\"Edited weight W =\")\n",
    "print(best_model_coef_edited)\n",
    "print(\"-\"*80)\n",
    "print(\"Deviations in Accuracy and Weight:\")\n",
    "print(\"Accuracy deviation = {}\".format(best_model_accuracy-best_model_accuracy_edited))\n",
    "print(\"W (weight) deviation:\")\n",
    "weight_deviation = np.abs(best_model_coef-best_model_coef_edited)\n",
    "print(weight_deviation)\n",
    "print(\"-\"*80)\n",
    "print(\"Top 4 features which have higher % change in weights compare to the other features:\")\n",
    "columns = list(data.columns)\n",
    "for index in np.argsort(-weight_deviation).reshape(-1)[:4]:\n",
    "    print(columns[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary above, we can see that the weight changes drastically before and after noise is added to the data. This indicates that there is multicolinearity in the data. It is important to note that multicolinearity does not affect models accuracy (as shown above). It does, however, affect the ability to accurately "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pertubation Test (Linear SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1e-05}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_SVM = SGDClassifier(loss=\"hinge\",random_state = 7)\n",
    "param = {\"alpha\" : [0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000]}\n",
    "clf = GridSearchCV(linear_SVM, param, cv=5)\n",
    "clf.fit(X, Y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model called best_model\n",
    "best_model_SVM = SGDClassifier(loss=\"log\",alpha=0.00001)\n",
    "best_model_SVM.fit(X,Y)\n",
    "y_pred = best_model_SVM.predict(X)\n",
    "best_model_accuracy_SVM = accuracy_score(Y,y_pred)\n",
    "best_model_coef_SVM = best_model_SVM.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modifying original data\n",
    "X_new = X + 0.01\n",
    "best_model_SVM.fit(X_new,Y)\n",
    "y_pred = best_model_SVM.predict(X_new)\n",
    "best_model_accuracy_edited_SVM = accuracy_score(Y,y_pred)\n",
    "best_model_coef_edited_SVM = best_model_SVM.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LINEAR SVM\n",
      "Original Accuracy = 1.0\n",
      "Edited Accuracy = 1.0\n",
      "--------------------------------------------------------------------------------\n",
      "Original weight W =\n",
      "[[ 10.78707214 -12.15260434  13.91198864  11.3911608  -12.15260434\n",
      "   11.93177473  -2.35828202]]\n",
      "Edited weight W =\n",
      "[[ 27.38163769 -18.41674525  56.64972584  26.06691492 -18.41674525\n",
      "   30.29522195  11.67445059]]\n",
      "--------------------------------------------------------------------------------\n",
      "Deviations in Accuracy and Weight:\n",
      "Accuracy deviation = 0.0\n",
      "W (weight) deviation:\n",
      "[[16.59456555  6.26414091 42.7377372  14.67575412  6.26414091 18.36344722\n",
      "  14.03273261]]\n",
      "--------------------------------------------------------------------------------\n",
      "Top 4 features which have higher % change in weights compare to the other features:\n",
      "z\n",
      "2*z+3*x*x\n",
      "x\n",
      "x*x\n"
     ]
    }
   ],
   "source": [
    "#compare result\n",
    "print(\"LINEAR SVM\")\n",
    "print(\"Original Accuracy = {}\".format(best_model_accuracy_SVM))\n",
    "print(\"Edited Accuracy = {}\".format(best_model_accuracy_edited_SVM))\n",
    "print(\"-\"*80)\n",
    "print(\"Original weight W =\")\n",
    "print(best_model_coef_SVM)\n",
    "print(\"Edited weight W =\")\n",
    "print(best_model_coef_edited_SVM)\n",
    "print(\"-\"*80)\n",
    "print(\"Deviations in Accuracy and Weight:\")\n",
    "print(\"Accuracy deviation = {}\".format(best_model_accuracy_SVM-best_model_accuracy_edited_SVM))\n",
    "print(\"W (weight) deviation:\")\n",
    "weight_deviation = np.abs(best_model_coef_SVM-best_model_coef_edited_SVM)\n",
    "print(weight_deviation)\n",
    "print(\"-\"*80)\n",
    "print(\"Top 4 features which have higher % change in weights compare to the other features:\")\n",
    "columns = list(data.columns)\n",
    "for index in np.argsort(-weight_deviation).reshape(-1)[:4]:\n",
    "    print(columns[index])\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "8D_LR_SVM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
